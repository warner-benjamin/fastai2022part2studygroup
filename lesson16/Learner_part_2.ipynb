{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from fastai.vision.all import *\n",
    "from fastai.metrics import ActivationType\n",
    "from fastprogress.fastprogress import format_time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner: Part 2\n",
    "\n",
    "In the past two lessons, we've defined a few iterations of a MiniAI Learner: a Basics Learner, Basic Callbacks Learner, and two different implementations of a Flexible Learner.\n",
    "\n",
    "The Lesson 16 Learner looks a bit more similar to the fastai Learner which we covered in the `Learner_part_1.ipynb` notebook then the Flexible Learner from Lesson 15.\n",
    "\n",
    "## MiniAI Flexible Callbacks\n",
    "\n",
    "This new version of the Flexible Learner calls callbacks via a Callback context manager `callback_ctx`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cbs(cbs, method_nm):\n",
    "    for cb in sorted(cbs, key=attrgetter('order')):\n",
    "        method = getattr(cb, method_nm, None)\n",
    "        if method is not None: method()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def callback_ctx(self, nm):\n",
    "    try:\n",
    "        self.callback(f'before_{nm}')\n",
    "        yield\n",
    "    except globals()[f'Cancel{nm.title()}Exception']: \n",
    "        pass\n",
    "    finally: \n",
    "        self.callback(f'after_{nm}')\n",
    "\n",
    "def callback(self, method_nm): \n",
    "    run_cbs(self.cbs, method_nm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which we then use in code like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, n_epochs):\n",
    "    with self.callback_ctx('fit'):\n",
    "        pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Flexible Learner also defines a `__getattr__` so we can define `self.predict()` in a Callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __getattr__(self, name):\n",
    "    if name in ('predict','get_loss','backward','step','zero_grad'): \n",
    "        return partial(self.callback, name)\n",
    "    raise AttributeError(name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works because a python class will use `__getattr__` to attempt to find any missing references when we call `self.predict()` without defining it in code. So the result is our MiniAI Learner will call all the Callback's `predict` methods, if they exist.\n",
    "\n",
    "## fastai Learner Callbacks\n",
    "\n",
    "This should all look quite familiar to what we covered in the Learner: Part 1 notebook, because it's an iteration on the same idea.\n",
    "\n",
    "In fastai instead of calling `self.predict` we'd call:\n",
    "\n",
    "```python\n",
    "self(\"after_create\")\n",
    "```\n",
    "\n",
    "this works because `Learner` overrides the standard Python `__call__` method to take an `event_name`. The event name(s) are mapped (passed) to `_call_one` which verifies the event is a valid callback event, sorts the callbacks, and then calls the callback's method for that event.\n",
    "\n",
    "Note: event is a fastcore `mk_class`, which effectively works as an Enum, but as a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __call__(self, event_name): \n",
    "    L(event_name).map(self._call_one)\n",
    "\n",
    "def _call_one(self, event_name):\n",
    "    if not hasattr(event, event_name): \n",
    "        raise Exception(f'missing {event_name}')\n",
    "    for cb in self.cbs.sorted('order'): \n",
    "        cb(event_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And fastai's Learner has an equivalent of `callback_ctx` called `_with_events` which calls both the \"before\" and \"after\" methods for each callback event and handles Callback errors via the \"after_cancel\" Callback event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _with_events(self, f, event_type, ex, final=noop):\n",
    "    try: \n",
    "        self(f'before_{event_type}')\n",
    "        f()\n",
    "    except ex: \n",
    "        self(f'after_cancel_{event_type}')\n",
    "    self(f'after_{event_type}')\n",
    "    final()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike MiniAI, `_with_events` allows us to handle an exception via the `after_cancel` Callback event. And the equivalent to `callback_ctx`'s Callback exception handling:\n",
    "```python\n",
    "except globals()[f'Cancel{nm.title()}Exception']: \n",
    "    pass\n",
    "```\n",
    "is in the `__call__` method of Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __call__(self, event_name):\n",
    "    if self.run: \n",
    "        try: \n",
    "            res = getcallable(self, event_name)()\n",
    "        except (CancelBatchException, \n",
    "                CancelBackwardException, \n",
    "                CancelEpochException, \n",
    "                CancelFitException, \n",
    "                CancelStepException, \n",
    "                CancelTrainException, \n",
    "                CancelValidException\n",
    "            ): \n",
    "                raise\n",
    "        except Exception as e: \n",
    "            raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loops"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MiniAI training loop is as follows, with \n",
    "```python\n",
    "with self.callback_ctx('fit')\n",
    "```\n",
    "and \n",
    "```python\n",
    "self.predict()\n",
    "```\n",
    "calling Callbacks as we previously discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, n_epochs):\n",
    "    self.n_epochs = n_epochs\n",
    "    self.epochs = range(n_epochs)\n",
    "    self.opt = self.opt_func(self.model.parameters(), self.lr)\n",
    "    with self.callback_ctx('fit'):\n",
    "        for self.epoch in self.epochs:\n",
    "            self.one_epoch(True)\n",
    "            with torch.no_grad(): \n",
    "                self.one_epoch(False)\n",
    "\n",
    "    def one_epoch(self, train):\n",
    "        self.model.train(train)\n",
    "        self.dl = self.dls.train if train else self.dls.valid\n",
    "        with self.callback_ctx('epoch'):\n",
    "            for self.iter,self.batch in enumerate(self.dl):\n",
    "                with self.callback_ctx('batch'):\n",
    "                    self.predict()\n",
    "                    self.get_loss()\n",
    "                    if self.model.training:\n",
    "                        self.backward()\n",
    "                        self.step()\n",
    "                        self.zero_grad()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile the fastai training loop looks like this (for details, see the Learner Part 1 notebook in the lesson 15 folder):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs optimizer, epoch, and hyperparameter setup. Then calls _with_events(_do_fit, 'fit')\n",
    "def fit(self, n_epoch, lr=None, wd=None, cbs=None, reset_opt=False, start_epoch=0):\n",
    "    pass\n",
    "\n",
    "def _do_fit(self): pass # Loops through all the epochs and calls _with_events(do_epoch, 'epoch')\n",
    "\n",
    "def _do_epoch(self): pass # Calls _do_epoch_train then _do_epoch_validate\n",
    "\n",
    "def _do_epoch_train(self): pass # Sets the Learner DataLoader to train and calls _with_events(all_batches, 'train')\n",
    "\n",
    "def all_batches(self): pass # Enumerates through all batches calling one_batch\n",
    "\n",
    "def one_batch(self, i, b): pass # Calls _set_device on the batch, optionally splits batch into Xs & Ys, and calls _with_events(_do_one_batch, 'batch')\n",
    "\n",
    "def _do_one_batch(self): pass # The tight training loop: prediction, loss, then calls _with_events(_backward, 'backward') & _with_events(self._step, 'step')\n",
    "\n",
    "def _backward(self): pass # Calls backwards on the loss\n",
    "\n",
    "def _step(self): pass # Calls the optimizer step. After this _do_one_batch calls opt.zero_grad\n",
    "\n",
    "def _do_epoch_valid(self): pass # Repeat the process on the validation set, minus the model update"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where instead of creating callback methods for `predict` and `backward` we have an `after_pred`, `after_loss`, and before and after backwards and step Callback `_with_events`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _do_one_batch(self):\n",
    "    self.pred = self.model(*self.xb)\n",
    "    self('after_pred')\n",
    "    if len(self.yb):\n",
    "        self.loss_grad = self.loss_func(self.pred, *self.yb)\n",
    "        self.loss = self.loss_grad.clone()\n",
    "    self('after_loss')\n",
    "    if not self.training or not len(self.yb): \n",
    "        return\n",
    "    self._with_events(self._backward, 'backward', CancelBackwardException)\n",
    "    self._with_events(self._step, 'step', CancelStepException)\n",
    "    self.opt.zero_grad()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Training Callbacks\n",
    "Like the MiniAI Learner, fastai's Learner defines training loop behavior in a set of default training Callbacks. Which we can see in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[fastai.callback.core.TrainEvalCallback,\n",
       " fastai.learner.Recorder,\n",
       " fastai.learner.CastToTensor,\n",
       " fastai.callback.progress.ProgressCallback]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "defaults.callbacks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will go over all four of these default training Callbacks. I will save Recorder for last, as I will discuss fastai Metrics and Recorder at the same time.\n",
    "\n",
    "### TrainEvalCallback\n",
    "\n",
    "First is the `TrainEvalCallback` Callback, which does a lot of setup, switches from the model from training to evaluation mode, and keeps track of and sets basic statistics, primarily for other Callbacks to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainEvalCallback(Callback):\n",
    "    \"`Callback` that tracks the number of iterations done and properly sets training/eval mode\"\n",
    "    order,run_valid = -10,False\n",
    "\n",
    "    def after_create(self): \n",
    "        self.learn.n_epoch = 1\n",
    "\n",
    "    def before_fit(self):\n",
    "        \"Set the iter and epoch counters to 0, put the model and the right device\"\n",
    "        self.learn.epoch,self.learn.loss = 0,tensor(0.)\n",
    "        self.learn.train_iter,self.learn.pct_train = 0,0.\n",
    "        device = getattr(self.dls, 'device', default_device())\n",
    "        self.model.to(device)\n",
    "        if isinstance(self.loss_func, (nn.Module, BaseLoss)): \n",
    "            self.loss_func.to(device)\n",
    "        if hasattr(self.model, 'reset'): \n",
    "            self.model.reset()\n",
    "\n",
    "    def after_batch(self):\n",
    "        \"Update the iter counter (in training mode)\"\n",
    "        self.learn.pct_train += 1./(self.n_iter*self.n_epoch)\n",
    "        self.learn.train_iter += 1\n",
    "\n",
    "    def before_train(self):\n",
    "        \"Set the model to training mode\"\n",
    "        self.learn.pct_train=self.epoch/self.n_epoch\n",
    "        self.model.train()\n",
    "        self.learn.training=True\n",
    "\n",
    "    def before_validate(self):\n",
    "        \"Set the model to validation mode\"\n",
    "        self.model.eval()\n",
    "        self.learn.training=False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ProgressCallback\n",
    "`ProgressCallback` in fastai behaves quite similarly to MiniAI's progress bar callback. Except it has two Progress Bars, one for Epochs and one for Batches, and prints Metrics in a table as training progresses. Like MiniAI, it sets the Learner's `logger` attribute for printing so other Callbacks like `Recorder` can use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressCallback(Callback):\n",
    "    \"A `Callback` to handle the display of progress bars\"\n",
    "    order,_stateattrs = 60,('mbar','pbar')\n",
    "\n",
    "    def before_fit(self):\n",
    "        assert hasattr(self.learn, 'recorder')\n",
    "        if self.create_mbar:\n",
    "            self.mbar = master_bar(list(range(self.n_epoch)))\n",
    "        if self.learn.logger != noop:\n",
    "            self.old_logger,self.learn.logger = self.logger,self._write_stats\n",
    "            self._write_stats(self.recorder.metric_names)\n",
    "        else: self.old_logger = noop\n",
    "\n",
    "    def before_epoch(self):\n",
    "        if getattr(self, 'mbar', False): \n",
    "            self.mbar.update(self.epoch)\n",
    "\n",
    "    def before_train(self):    \n",
    "        self._launch_pbar()\n",
    "\n",
    "    def before_validate(self): \n",
    "        self._launch_pbar()\n",
    "\n",
    "    def after_train(self):     \n",
    "        self.pbar.on_iter_end()\n",
    "\n",
    "    def after_validate(self):  \n",
    "        self.pbar.on_iter_end()\n",
    "\n",
    "    def after_batch(self):\n",
    "        self.pbar.update(self.iter+1)\n",
    "        if hasattr(self, 'smooth_loss'): \n",
    "            self.pbar.comment = f'{self.smooth_loss:.4f}'\n",
    "\n",
    "    def _launch_pbar(self):\n",
    "        self.pbar = progress_bar(self.dl, parent=getattr(self, 'mbar', None), leave=False)\n",
    "        self.pbar.update(0)\n",
    "\n",
    "    def after_fit(self):\n",
    "        if getattr(self, 'mbar', False):\n",
    "            self.mbar.on_iter_end()\n",
    "            delattr(self, 'mbar')\n",
    "        if hasattr(self, 'old_logger'): \n",
    "            self.learn.logger = self.old_logger\n",
    "\n",
    "    def _write_stats(self, log):\n",
    "        if getattr(self, 'mbar', False): \n",
    "            self.mbar.write([f'{l:.6f}' if isinstance(l, float) else str(l) for l in log], table=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CastToTensor\n",
    "`CastToTensor` is a very simple callback which casts any fastai subclassed Tensors (such as `TensorImage`) to a normal Tensor before passing them to the model.\n",
    "\n",
    "Training on a subclassed Tensor can result in up to a forty percent decrease in GPU throughput when using Automatic Mixed Precision and Channels Last memory format on modern hardware. Thomas Capelle and I discovered this bug, which you can [read more about here](https://benjaminwarner.dev/2022/06/14/debugging-pytorch-performance-decrease.html).\n",
    "\n",
    "This one is important to note as any Callback which needs to make use of the Tensor subclass type must occur before order of 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cast_tensor(x): \n",
    "    if isinstance(x, tuple): \n",
    "        return tuple(_cast_tensor(x_) for x_ in x)\n",
    "    else: \n",
    "        return cast(x, Tensor) if isinstance(x,torch.Tensor) else x\n",
    "\n",
    "class CastToTensor(Callback):\n",
    "    \"Cast Subclassed Tensors to `Tensor`\"\n",
    "    order=9 # Right before MixedPrecision\n",
    "\n",
    "    def before_batch(self):\n",
    "        self.learn.xb = _cast_tensor(self.learn.xb)\n",
    "        self.learn.yb = _cast_tensor(self.learn.yb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and Recorder\n",
    "fastai has its own metrics system. At the time of development, `TorchMetrics` from Lightning.ai, `TorchEval` from PyTorch, and `Evaluate` from Hugging Face all didn't exist, so fastai built its own metrics system with a combination of custom code and scikit-learn metrics.\n",
    "\n",
    "All fastai metrics inherit from the base `Metric` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metric():\n",
    "    \"Blueprint for defining a metric\"\n",
    "    def reset(self):\n",
    "        \"Reset inner state to prepare for new computation\"\n",
    "        pass\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        \"Use `learn` to update the state with new results\"\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        \"The value of the metric\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        \"Name of the `Metric`, camel-cased and with Metric removed\" \n",
    "        return class2attr(self, 'Metric')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AvgMetric` is the default fastai metric. It accepts a function `func` which it then calculates and stores the metric when `accumulate` and `value` are called.\n",
    "\n",
    "If you pass a functional metric to Learner\n",
    "\n",
    "```python\n",
    "Learner(..., metrics=accuracy)\n",
    "```\n",
    "it will automatically be converted to an `AvgMetric` behind the scenes. This behavior can cause issues if your metric cannot be averaged across batches. For example, the mean of multiple batches of Root Means Square Error isn't equal to the RMSE of the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgMetric(Metric):\n",
    "    \"Average the values of `func` taking into account potential different batch sizes\"\n",
    "    def __init__(self, func): \n",
    "        self.func = func\n",
    "\n",
    "    def reset(self): \n",
    "        self.total,self.count = 0.,0\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        bs = find_bs(learn.yb)\n",
    "        self.total += learn.to_detach(self.func(learn.pred, *learn.yb))*bs\n",
    "        self.count += bs\n",
    "\n",
    "    @property\n",
    "    def value(self): \n",
    "        return self.total/self.count if self.count != 0 else None\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.func.func.__name__ if hasattr(self.func, 'func') else  self.func.__name__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastai also can keep track of the Loss as a metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgLoss(Metric):\n",
    "    \"Average the losses taking into account potential different batch sizes\"\n",
    "    def reset(self):           \n",
    "        self.total,self.count = 0.,0\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        bs = find_bs(learn.yb)\n",
    "        self.total += learn.to_detach(learn.loss.mean())*bs\n",
    "        self.count += bs\n",
    "\n",
    "    @property\n",
    "    def value(self): \n",
    "        return self.total/self.count if self.count != 0 else None\n",
    "\n",
    "    @property\n",
    "    def name(self):  \n",
    "        return \"loss\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the exponentially smoothed loss, which is what is displayed in `Recorder` by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgSmoothLoss(Metric):\n",
    "    \"Smooth average of the losses (exponentially weighted with `beta`)\"\n",
    "    def __init__(self, beta=0.98): \n",
    "        self.beta = beta\n",
    "\n",
    "    def reset(self):               \n",
    "        self.count,self.val = 0,tensor(0.)\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        self.count += 1\n",
    "        self.val = torch.lerp(to_detach(learn.loss.mean()), self.val, self.beta)\n",
    "\n",
    "    @property\n",
    "    def value(self): \n",
    "        return self.val/(1-self.beta**self.count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AccumMetric` is the answer for the RMSE problem and how to use scikit-learn metrics since they are implemented using NumPy as the backend and our results are in Tensors.\n",
    "\n",
    "Instead of calculating the metric and averaging it as we go, `AccumMetric` accumulates all the values, applying any `activation` and thresholding `thresh` along the way (removed for brevity, see the source code for full details). When `value` is called, `AccumMetric` calculates the metric on all the batches at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class AccumMetric(Metric):\n",
    "    \"Stores predictions and targets on CPU in accumulate to perform final calculations with `func`.\"\n",
    "    def __init__(self, func, dim_argmax=None, activation=ActivationType.No, thresh=None, to_np=False,\n",
    "                 invert_arg=False, flatten=True, name=None, **kwargs):\n",
    "        store_attr('func,dim_argmax,activation,thresh,flatten')\n",
    "        self._name = ifnone(name, self.func.func.__name__ if hasattr(self.func, 'func') else  self.func.__name__)\n",
    "        self.to_np,self.invert_args,self.kwargs = to_np,invert_arg,kwargs\n",
    "\n",
    "    def reset(self):\n",
    "        \"Clear all targs and preds\"\n",
    "        self.targs,self.preds = [],[]\n",
    "\n",
    "    def accumulate(self, learn):\n",
    "        \"Store targs and preds from `learn`, using activation function and argmax as appropriate\"\n",
    "        pred = learn.pred\n",
    "        # handle activations here\n",
    "        self.accum_values(pred,learn.y,learn)\n",
    "\n",
    "    def accum_values(self, preds, targs,learn=None):\n",
    "        \"Store targs and preds\"\n",
    "        to_d = learn.to_detach if learn is not None else to_detach\n",
    "        preds,targs = to_d(preds),to_d(targs)\n",
    "        if self.flatten: \n",
    "            preds,targs = flatten_check(preds,targs)\n",
    "        self.preds.append(preds)\n",
    "        self.targs.append(targs)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        \"Value of the metric using accumulated preds and targs\"\n",
    "        if len(self.preds) == 0: return\n",
    "        preds,targs = torch.cat(self.preds),torch.cat(self.targs)\n",
    "        if self.to_np: \n",
    "            preds,targs = preds.numpy(),targs.numpy()\n",
    "        return self.func(targs, preds, **self.kwargs) if self.invert_args else self.func(preds, targs, **self.kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I reproduced the documetation on `AccumMetric` here for some more details on `AccumMetric`\n",
    "> `func` is only applied to the accumulated predictions/targets when the `value` attribute is asked for (so at the end of a validation/training phase, in use with `Learner` and its `Recorder`).The signature of `func` should be `inp,targ` (where `inp` are the predictions of the model and `targ` the corresponding labels).\n",
    "> \n",
    "> For classification problems with single label, predictions need to be transformed with a softmax then an argmax before being compared to the targets. Since a softmax doesn't change the order of the numbers, we can just apply the argmax. Pass along `dim_argmax` to have this done by `AccumMetric` (usually -1 will work pretty well). If you need to pass to your metrics the probabilities and not the predictions, use `softmax=True`.\n",
    "> \n",
    "> For classification problems with multiple labels, or if your targets are one-hot encoded, predictions may need to pass through a sigmoid (if it wasn't included in your model) then be compared to a given threshold (to decide between 0 and 1), this is done by `AccumMetric` if you pass `sigmoid=True` and/or a value for `thresh`.\n",
    "> \n",
    "> If you want to use a metric function sklearn.metrics, you will need to convert predictions and labels to numpy arrays with `to_np=True`. Also, scikit-learn metrics adopt the convention `y_true`, `y_preds` which is the opposite from us, so you will need to pass `invert_arg=True` to make `AccumMetric` do the inversion for you.\n",
    "\n",
    "Finally, there is a convience method for creating a fastai metric from scikit-learn using `AccumMetric`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skm_to_fastai(func, is_class=True, thresh=None, axis=-1, activation=None, **kwargs):\n",
    "    \"Convert `func` from sklearn.metrics to a fastai metric\"\n",
    "    dim_argmax = axis if is_class and thresh is None else None\n",
    "    if activation is None:\n",
    "        activation = ActivationType.Sigmoid if (is_class and thresh is not None) else ActivationType.No\n",
    "    return AccumMetric(func, dim_argmax=dim_argmax, activation=activation, thresh=thresh,\n",
    "                       to_np=True, invert_arg=True, **kwargs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recorder\n",
    "\n",
    "`Recorder` is the Callback which records all the metrics allowing us and loggers to capture and view them. Recorder can either record all metrics on train, valid, or both, without any granularity. This basic metric setup, like grabbing metric names, all happens in `before_fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recorder(Callback):\n",
    "    \"Callback that registers statistics (lr, loss and metrics) during training\"\n",
    "    _stateattrs=('lrs','iters','losses','values')\n",
    "    remove_on_fetch,order = True,50\n",
    "\n",
    "    def __init__(self, add_time=True, train_metrics=False, valid_metrics=True, beta=0.98):\n",
    "        store_attr('add_time,train_metrics,valid_metrics')\n",
    "        self.loss,self.smooth_loss = AvgLoss(),AvgSmoothLoss(beta=beta)\n",
    "\n",
    "    def before_fit(self):\n",
    "        \"Prepare state for training\"\n",
    "        self.lrs,self.iters,self.losses,self.values = [],[],[],[]\n",
    "        names = self.metrics.attrgot('name')\n",
    "        if self.train_metrics and self.valid_metrics:\n",
    "            names = L('loss') + names\n",
    "            names = names.map('train_{}') + names.map('valid_{}')\n",
    "        elif self.valid_metrics: \n",
    "            names = L('train_loss', 'valid_loss') + names\n",
    "        else: \n",
    "            names = L('train_loss') + names\n",
    "        if self.add_time: \n",
    "            names.append('time')\n",
    "        self.metric_names = 'epoch'+names\n",
    "        self.smooth_loss.reset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All `Metrics` need to be reset before recording metrics for each epoch, which is handled in `before_train` and `before_validate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def before_train(self): \n",
    "    self._train_mets[1:].map(Self.reset())\n",
    "\n",
    "def before_validate(self): \n",
    "    self._valid_mets.map(Self.reset())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`after_batch` calls the `accumulate` method of all the metrics \n",
    "```python\n",
    "for met in mets: \n",
    "    met.accumulate(self.learn)\n",
    "```\n",
    "but only if there are labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def after_batch(self):\n",
    "    \"Update all metrics and records lr and smooth loss in training\"\n",
    "    if len(self.yb) == 0: return\n",
    "    mets = self._train_mets if self.training else self._valid_mets\n",
    "    for met in mets: \n",
    "        met.accumulate(self.learn)\n",
    "    if not self.training: \n",
    "        return\n",
    "    self.lrs.append(self.opt.hypers[-1]['lr'])\n",
    "    self.losses.append(self.smooth_loss.value)\n",
    "    self.learn.smooth_loss = self.smooth_loss.value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we're done with a train/valid epoch, `Recorder` appends the metrics into a list `log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _maybe_item(t):\n",
    "    t = t.value\n",
    "    try: return t.item()\n",
    "    except: return t\n",
    "\n",
    "def after_train(self): \n",
    "    self.log += self._train_mets.map(_maybe_item)\n",
    "\n",
    "def after_validate(self): \n",
    "    self.log += self._valid_mets.map(_maybe_item)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then at the end of the epoch, does the same except for epoch statistics like `self.logger(self.log)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def after_epoch(self):\n",
    "    \"Store and log the loss/metric values\"\n",
    "    self.learn.final_record = self.log[1:].copy()\n",
    "    self.values.append(self.learn.final_record)\n",
    "    if self.add_time: \n",
    "        self.log.append(format_time(time.time() - self.start_epoch))\n",
    "    self.logger(self.log)\n",
    "    self.iters.append(self.smooth_loss.count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastxtend Metrics\n",
    "\n",
    "I created an improved version of fastai metrics as a part of [fastxtend](https://fastxtend.benjaminwarner.dev) which are backwards compatible with fastai metrics.\n",
    "\n",
    "1. fastxtend metrics can independently log on train, valid, or both train and valid\n",
    "2. All fastxtend metrics can use the activation support of fastai's `AccumMetric`, inherited from `MetricX`\n",
    "3. fastxtend metrics add `AvgSmoothMetric`, a metric version of `AvgSmoothLoss`\n",
    "\n",
    "I use it mostly for [logging multiple losses](https://fastxtend.benjaminwarner.dev/multiloss.html) individually as metrics. You can check out [fastxtend metrics here](https://fastxtend.benjaminwarner.dev/metrics.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:45:29) \n[GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13823a492421f5f84f9059b75c88ebb0eec72237eae812caa6d0bf5be399ae62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
