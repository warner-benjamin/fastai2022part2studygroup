{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from typing import Iterable, List, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from torch.optim.optimizer import required  # type: ignore\n",
    "\n",
    "from timm.optim.optim_factory import create_optimizer_v2, param_groups_weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = torch.Tensor()\n",
    "learning_rate = 3e-3\n",
    "model = nn.Module()\n",
    "list_of_params_without_wd = [model.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by looking at Composer's `DecoupledSGDW`, as it fastai's SGD with momentum and decoupled weight decay and is less complicated then PyTorch's `SGD`.\n",
    "\n",
    "Remember that a basic SGD step is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = param + param.grad * -learning_rate\n",
    "\n",
    "# Or written using PyTorch ops\n",
    "\n",
    "param = param.add(param.grad, alpha=-learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll look at each part step by step, with Nesterov momentum, `initial_lr`, clouture, and extra comments removed to simplify the code a bit.\n",
    "\n",
    "An optimizer is initialized with the parameters we want to train, from any number of models with gradients, and the optimizer specific hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoupledSGDW(SGD):\n",
    "    \"\"\"SGD optimizer with the weight decay term decoupled from the learning rate.\n",
    "\n",
    "    Args:\n",
    "        params (iterable): Iterable of parameters to optimize or dicts defining parameter groups.\n",
    "        lr (float): Learning rate.\n",
    "        momentum (int, optional): Momentum factor. Default: ``0``.\n",
    "        dampening (int, optional): Dampening factor applied to the momentum. Default: ``0``.\n",
    "        weight_decay (int, optional): Decoupled weight decay factor. Default: ``0``.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 params: Union[Iterable[torch.Tensor], Iterable[dict]],\n",
    "                 lr: float = required,\n",
    "                 momentum: float = 0,\n",
    "                 dampening: float = 0,\n",
    "                 weight_decay: float = 0):\n",
    "\n",
    "        super().__init__(params=params,\n",
    "                         lr=lr,\n",
    "                         momentum=momentum,\n",
    "                         dampening=dampening,\n",
    "                         weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`params` can also be parameter groups, which are dictionaries with default values for that parameter group. So, if we wanted to not apply weight decay to normalization layers or bias terms (which is usually a good idea), we'd create two parameter groups, one with normal model parameters and one with our normalization layers or bias term parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for conciseness, I omit the model parameters, which are required to be part of the \n",
    "parameter_groups = [\n",
    "    dict(weight_decay=0.),\n",
    "    dict(weight_decay=1e-2)\n",
    "]\n",
    "\n",
    "# or more likely, we'd use a method to do it for us. Like this param_groups_weight_decay from timm\n",
    "parameter_groups = param_groups_weight_decay(model, 1e-2, list_of_params_without_wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall in the training loop the optimization step occurs after we calculate the gradients from the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb,yb in train_dl:\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer step function will loop through all parameter groups and collect parameter groups default values, then loop through all parameters and apply the optimization step to any parameter with a gradient. (Frozen model layers will not have gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def step(self):\n",
    "    \"Performs a single optimization step.\"\n",
    "    for group in self.param_groups:\n",
    "        params_with_grad = []\n",
    "        grad_list = []\n",
    "        momentum_buffer_list = []\n",
    "        weight_decay = group['weight_decay']\n",
    "        momentum = group['momentum']\n",
    "        dampening = group['dampening']\n",
    "        lr = group['lr']\n",
    "\n",
    "        for p in group['params']:\n",
    "            if p.grad is not None:\n",
    "                params_with_grad.append(p)\n",
    "                grad_list.append(p.grad)\n",
    "\n",
    "                state = self.state[p]\n",
    "                if 'momentum_buffer' not in state:\n",
    "                    momentum_buffer_list.append(None)\n",
    "                else:\n",
    "                    momentum_buffer_list.append(state['momentum_buffer'])\n",
    "\n",
    "        self.sgdw(params_with_grad,\n",
    "                    grad_list,\n",
    "                    momentum_buffer_list,\n",
    "                    weight_decay=weight_decay,\n",
    "                    momentum=momentum,\n",
    "                    lr=lr,\n",
    "                    dampening=dampening)\n",
    "\n",
    "        # update momentum_buffers in state\n",
    "        for p, momentum_buffer in zip(params_with_grad, momentum_buffer_list):\n",
    "            state = self.state[p]\n",
    "            state['momentum_buffer'] = momentum_buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we apply the optimizer step to the parameters using the gradients and our hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@staticmethod\n",
    "def sgdw(params: List[torch.Tensor], grad_list: List[torch.Tensor], momentum_buffer_list: List[torch.Tensor], *,\n",
    "            weight_decay: float, momentum: float, lr: float, initial_lr: float, dampening: float, nesterov: bool):\n",
    "    \"Functional API that performs SGDW algorithm computation.\"\n",
    "    for i, param in enumerate(params):\n",
    "        grad = grad_list[i]\n",
    "        \n",
    "        if momentum != 0:\n",
    "            buf = momentum_buffer_list[i]\n",
    "\n",
    "            if buf is None:\n",
    "                buf = torch.clone(grad).detach()\n",
    "                momentum_buffer_list[i] = buf\n",
    "            else:\n",
    "                buf.mul_(momentum).add_(grad, alpha=1 - dampening)\n",
    "            grad = buf\n",
    "\n",
    "        if weight_decay != 0:\n",
    "            param.mul_(1 - lr * weight_decay)\n",
    "\n",
    "        param.add_(grad, alpha=-lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full Composer `DecoupledSGDW` optimizer (with extra comments and warnings removed), which is equivalent to fastai's SGD with momentum and decoupled weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoupledSGDW(SGD):\n",
    "    def __init__(self,\n",
    "                 params: Union[Iterable[torch.Tensor], Iterable[dict]],\n",
    "                 lr: float = required,\n",
    "                 momentum: float = 0,\n",
    "                 dampening: float = 0,\n",
    "                 weight_decay: float = 0,\n",
    "                 nesterov: bool = False):\n",
    "        super().__init__(params=params,\n",
    "                         lr=lr,\n",
    "                         momentum=momentum,\n",
    "                         dampening=dampening,\n",
    "                         weight_decay=weight_decay,\n",
    "                         nesterov=nesterov)\n",
    "        for group in self.param_groups:\n",
    "            group['initial_lr'] = group['lr']\n",
    "\n",
    "    @staticmethod\n",
    "    def sgdw(params: List[torch.Tensor], grad_list: List[torch.Tensor], momentum_buffer_list: List[torch.Tensor], *,\n",
    "             weight_decay: float, momentum: float, lr: float, initial_lr: float, dampening: float, nesterov: bool):\n",
    "        \"Functional API that performs SGDW algorithm computation.\"\n",
    "        for i, param in enumerate(params):\n",
    "\n",
    "            grad = grad_list[i]\n",
    "\n",
    "            if momentum != 0:\n",
    "                buf = momentum_buffer_list[i]\n",
    "\n",
    "                if buf is None:\n",
    "                    buf = torch.clone(grad).detach()\n",
    "                    momentum_buffer_list[i] = buf\n",
    "                else:\n",
    "                    buf.mul_(momentum).add_(grad, alpha=1 - dampening)\n",
    "\n",
    "                if nesterov:\n",
    "                    grad = grad.add(buf, alpha=momentum)\n",
    "                else:\n",
    "                    grad = buf\n",
    "\n",
    "            if weight_decay != 0:\n",
    "                decay_factor = (lr / initial_lr) if initial_lr else 1.0\n",
    "                param.mul_(1 - decay_factor * weight_decay)\n",
    "\n",
    "            param.add_(grad, alpha=-lr)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"Performs a single optimization step.\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            params_with_grad = []\n",
    "            grad_list = []\n",
    "            momentum_buffer_list = []\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            dampening = group['dampening']\n",
    "            nesterov = group['nesterov']\n",
    "            lr = group['lr']\n",
    "            initial_lr = group['initial_lr']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    params_with_grad.append(p)\n",
    "                    grad_list.append(p.grad)\n",
    "\n",
    "                    state = self.state[p]\n",
    "                    if 'momentum_buffer' not in state:\n",
    "                        momentum_buffer_list.append(None)\n",
    "                    else:\n",
    "                        momentum_buffer_list.append(state['momentum_buffer'])\n",
    "\n",
    "            self.sgdw(params_with_grad,\n",
    "                      grad_list,\n",
    "                      momentum_buffer_list,\n",
    "                      weight_decay=weight_decay,\n",
    "                      momentum=momentum,\n",
    "                      lr=lr,\n",
    "                      initial_lr=initial_lr,\n",
    "                      dampening=dampening,\n",
    "                      nesterov=nesterov)\n",
    "\n",
    "            # update momentum_buffers in state\n",
    "            for p, momentum_buffer in zip(params_with_grad, momentum_buffer_list):\n",
    "                state = self.state[p]\n",
    "                state['momentum_buffer'] = momentum_buffer\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at PyTorch's SGD, to see additional features like for each methods, nestorov momentum, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fastai Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastai.optimizer import _BaseOptimizer, _update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastai optimizers a bit different.\n",
    "\n",
    "First, every hyperparameter can be set per parameter or parameter group.\n",
    "\n",
    "Second, optimizers are defined as optim step callbacks, which are called by `Optimizer` one after another.\n",
    "\n",
    "Third, they support discriminative learning rates out of the box. i.e. different learning rates per parameter group.\n",
    "\n",
    "Like PyTorch optimizers, `Optimizer` is initialized with the parameters we want to train and the optimizer specific hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(_BaseOptimizer):\n",
    "    \"Base optimizer class for the fastai library, updating `params` with `cbs`\"\n",
    "    _keep_on_clear = ['force_train', 'do_wd']\n",
    "    def __init__(self,\n",
    "        params:Tensor, # Model parameters or parameter groups\n",
    "        cbs:list, # `Optimizer` step callbacks\n",
    "        **defaults # Hyper parameters default values\n",
    "    ):\n",
    "        params = L(params)\n",
    "        self.cbs,self.state = L(cbs),defaultdict(dict)\n",
    "        defaults = merge(*self.cbs.attrgot('defaults'), defaults)\n",
    "        self.param_lists = L(L(p) for p in params) if isinstance(params[0], (L,list)) else L([params])\n",
    "        self.hypers = L({} for _ in range_of(self.param_lists))\n",
    "        self.set_hypers(**defaults)\n",
    "        self.frozen_idx = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And optimizer step function loops through all parameter groups and collect parameter groups default values, and then loop through all parameters and apply the optimization step to any parameter with a gradient.\n",
    "\n",
    "Except there is an additional loop for optimizer callbacks, which apply the optimizer steps and update the state dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_params(self,\n",
    "    with_grad:bool=False # Get all parameters. If `True` select only those with a gradient\n",
    "):\n",
    "    res = L((p,pg,self.state[p],hyper) for pg,hyper in zip(self.param_lists,self.hypers) for p in pg)\n",
    "    return L(o for o in res if hasattr(o[0], 'grad') and o[0].grad is not None) if with_grad else res\n",
    "\n",
    "\n",
    "def step(self):\n",
    "    for p,pg,state,hyper in self.all_params(with_grad=True):\n",
    "        for cb in self.cbs: \n",
    "            state = _update(state, cb(p, **{**state, **hyper}))\n",
    "        self.state[p] = state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simplified SGD (showing only SGDW decoupled true weight decay). \n",
    "\n",
    "Note all the separate optimizer steps defined as callbacks added to `cbs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(\n",
    "    params:Tensor, # Model parameters or parameter groups\n",
    "    lr:float, # Default learning rate\n",
    "    mom:float=0., # Gradient moving average (β1) coefficient\n",
    "    wd:float=0., # Optional weight decay (true or L2)\n",
    ") -> Optimizer:\n",
    "    \"A `Optimizer` for SGD with `lr` and `mom` and `params`\"\n",
    "    cbs = [weight_decay]\n",
    "    if mom != 0: cbs.append(average_grad)\n",
    "    cbs.append(sgd_step if mom==0 else momentum_step)\n",
    "    return Optimizer(params, cbs, lr=lr, mom=mom, wd=wd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And all the optimizer callback steps (simplified a bit), which are almost the same as `DecoupledSGDW` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_decay(p, lr, wd, do_wd=True, **kwargs):\n",
    "    \"Weight decay as decaying `p` with `lr*wd`\"\n",
    "    if do_wd and wd!=0: \n",
    "        p.data.mul_(1 - lr*wd)\n",
    "\n",
    "\n",
    "def average_grad(p, mom, grad_avg=None, **kwargs):\n",
    "    \"Keeps track of the avg grads of `p` in `state` with `mom`.\"\n",
    "    if grad_avg is None: \n",
    "        grad_avg = torch.zeros_like(p.grad.data)\n",
    "\n",
    "    grad_avg.mul_(mom).add_(p.grad.data)\n",
    "    return {'grad_avg': grad_avg}\n",
    "\n",
    "\n",
    "def momentum_step(p, lr, grad_avg, **kwargs):\n",
    "    \"Step for SGD with momentum with `lr`\"\n",
    "    p.data.add_(grad_avg, alpha=-lr)\n",
    "\n",
    "\n",
    "def sgd_step(p, lr, **kwargs):\n",
    "    \"Step for SGD with `lr`\"\n",
    "    p.data.add_(p.grad.data, alpha=-lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full `Optimizer` class with `zero_grad`, loading, and clearing, the `state_dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(_BaseOptimizer):\n",
    "    \"Base optimizer class for the fastai library, updating `params` with `cbs`\"\n",
    "    _keep_on_clear = ['force_train', 'do_wd']\n",
    "    def __init__(self,\n",
    "        params:Tensor, # Model parameters or parameter groups\n",
    "        cbs:list, # `Optimizer` step callbacks\n",
    "        **defaults # Hyper parameters default values\n",
    "    ):\n",
    "        params = L(params)\n",
    "        self.cbs,self.state = L(cbs),defaultdict(dict)\n",
    "        defaults = merge(*self.cbs.attrgot('defaults'), defaults)\n",
    "        self.param_lists = L(L(p) for p in params) if isinstance(params[0], (L,list)) else L([params])\n",
    "        self.hypers = L({} for _ in range_of(self.param_lists))\n",
    "        self.set_hypers(**defaults)\n",
    "        self.frozen_idx = 0\n",
    "\n",
    "    def zero_grad(self):\n",
    "        for p,*_ in self.all_params(with_grad=True):\n",
    "            p.grad.detach_()\n",
    "            p.grad.zero_()\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        if closure is not None: raise NotImplementedError(\"fastai optimizers currently do not support closure\")\n",
    "        for p,pg,state,hyper in self.all_params(with_grad=True):\n",
    "            for cb in self.cbs: state = _update(state, cb(p, **{**state, **hyper}))\n",
    "            self.state[p] = state\n",
    "\n",
    "    def clear_state(self):\n",
    "        for p,pg,state,hyper in self.all_params():\n",
    "            self.state[p] = {k: state[k] for k in self._keep_on_clear if k in state}\n",
    "\n",
    "    def state_dict(self):\n",
    "        state = [self.state[p] for p,*_ in self.all_params()]\n",
    "        return {'state': state, 'hypers': self.hypers}\n",
    "\n",
    "    def load_state_dict(self,\n",
    "        sd:dict # State dict with `hypers` and `state` to load on the optimizer\n",
    "    ):\n",
    "        assert len(sd[\"hypers\"]) == len(self.param_lists)\n",
    "        assert len(sd[\"state\"])  == sum([len(pg) for pg in self.param_lists])\n",
    "        self.hypers = sd['hypers']\n",
    "        self.state = {p: s for p,s in zip(self.all_params().itemgot(0), sd['state'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting hyper parameters, freezing and unfreezing, and compatibility with PyTorch optimizers are all part of `_BaseOptimizer`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('fastai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13823a492421f5f84f9059b75c88ebb0eec72237eae812caa6d0bf5be399ae62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
