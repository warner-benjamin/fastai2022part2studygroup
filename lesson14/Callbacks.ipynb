{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastai.callback.core import _events, _inner_loop\n",
    "from torch.cuda.amp import GradScaler,autocast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the full minimal training loop from the lesson.\n",
    "\n",
    "fastai callbacks can be used to modify this training loop without overcomplicating the training loop code. This also allows us to easily mix and match different callbacks with each other and keep the compatibility code between training methods outside of the main training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb,yb in train_dl:\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tot_loss,tot_acc,count = 0.,0.,0\n",
    "            for xb,yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                n = len(xb)\n",
    "                count += n\n",
    "                tot_loss += loss_func(pred,yb).item()*n\n",
    "                tot_acc  += accuracy (pred,yb).item()*n\n",
    "        print(epoch, tot_loss/count, tot_acc/count)\n",
    "    return tot_loss/count, tot_acc/count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A callback can implement actions on the following events:\n",
    "\n",
    "- `after_create`: called after the `Learner` is created\n",
    "- `before_fit`: called before starting training or inference, ideal for initial setup.\n",
    "- `before_epoch`: called at the beginning of each epoch, useful for any behavior you need to reset at each epoch.\n",
    "- `before_train`: called at the beginning of the training part of an epoch.\n",
    "- `before_batch`: called at the beginning of each batch, just after drawing said batch. It can be used to do any setup necessary for the batch (like hyper-parameter scheduling) or to change the input/target before it goes in the model (change of the input with techniques like mixup for instance).\n",
    "- `after_pred`: called after computing the output of the model on the batch. It can be used to change that output before it's fed to the loss.\n",
    "- `after_loss`: called after the loss has been computed, but before the backward pass. It can be used to add any penalty to the loss (AR or TAR in RNN training for instance).\n",
    "- `before_backward`: called after the loss has been computed, but only in training mode (i.e. when the backward pass will be used)\n",
    "- `after_backward`: called after the backward pass, but before the update of the parameters. Generally `before_step` should be used instead.\n",
    "- `before_step`: called after the backward pass, but before the update of the parameters. It can be used to do any change to the gradients before said update (gradient clipping for instance).\n",
    "- `after_step`: called after the step and before the gradients are zeroed.\n",
    "- `after_batch`: called at the end of a batch, for any clean-up before the next one.\n",
    "- `after_train`: called at the end of the training phase of an epoch.\n",
    "- `before_validate`: called at the beginning of the validation phase of an epoch, useful for any setup needed specifically for validation.\n",
    "- `after_validate`: called at the end of the validation part of an epoch.\n",
    "- `after_epoch`: called at the end of an epoch, for any clean-up before the next one.\n",
    "- `after_fit`: called at the end of training, for final clean-up.\n",
    "\n",
    "(the above is excepted from the fastai docs on Callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every callback has:\n",
    " - `order`: defines when the callback is ran when called\n",
    " - `learn`: easy access to `Learner` via `self.learn`\n",
    " - `run`: controls if the callback runs\n",
    " - `run_train`: controls if the callback runs on training loop events\n",
    " - `run_train`: controls if the callback runs on validation loop events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@funcs_kwargs(as_method=True)\n",
    "class Callback(Stateful,GetAttr):\n",
    "    \"Basic class handling tweaks of the training loop by changing a `Learner` in various events\"\n",
    "    order = 0\n",
    "    learn = None\n",
    "    run = True\n",
    "    run_train = True\n",
    "    run_valid = True\n",
    "    _methods = _events\n",
    "\n",
    "    def __init__(self, **kwargs): \n",
    "        assert not kwargs, f'Passed unknown events: {kwargs}' # I think this should state unknown arguments, not events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `__call__` method determines if an individual callback is called using `run`, `run_train`, and `run_valid`.\n",
    "\n",
    "Note the following events:\n",
    "\n",
    "`after_create`, `before_fit`, `before_epoch`, `before_train`, `after_train`, `before_validate`, `after_validate`, `after_epoch`, `after_fit`\n",
    "\n",
    "are always called by `Learner`.\n",
    "\n",
    "There are also `after_cancel_*` methods for all callback events that have a fastai cancel exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __call__(self, event_name):\n",
    "    \"Call `self.{event_name}` if it's defined\"\n",
    "    _run = (event_name not in _inner_loop or (self.run_train and getattr(self, 'training', True)) or\n",
    "            (self.run_valid and not getattr(self, 'training', False)))\n",
    "    res = None\n",
    "    if self.run and _run: \n",
    "        try: \n",
    "            res = getcallable(self, event_name)()\n",
    "        except (CancelBatchException, \n",
    "                CancelBackwardException, \n",
    "                CancelEpochException, \n",
    "                CancelFitException, \n",
    "                CancelStepException, \n",
    "                CancelTrainException, \n",
    "                CancelValidException\n",
    "            ): \n",
    "                raise\n",
    "        except Exception as e: \n",
    "            raise modify_exception(e, f'Exception occured in `{self.__class__.__name__}` when calling event `{event_name}`:\\n\\t{e.args[0]}', replace=True)\n",
    "    if event_name=='after_fit': \n",
    "        self.run=True #Reset self.run to True at each end of fit\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to inheriting from `GetAttr`:\n",
    "\n",
    "```ptyhon\n",
    "class Callback(Stateful,GetAttr):\n",
    "```\n",
    "\n",
    "we can read any `Learner` attribute from `self.var` instead of `self.learn.var`. Although to write to those attributes, we should use `self.learn.var` and not `self.var`.\n",
    "\n",
    "This is enforced via the `__setattr__` in `Callback`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __setattr__(self, name, value):\n",
    "    \"Set an attribute for a `Callback`\"\n",
    "    if hasattr(self.learn,name):\n",
    "        warn(f\"You are shadowing an attribute ({name}) that exists in the learner. Use `self.learn.{name}` to avoid this\")\n",
    "    super().__setattr__(name, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callback Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastai uses callbacks to simplify and extend the training loop. For example, setting the model to train or eval is handled by the `TrainEvalCallback`. Along with basic training statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainEvalCallback(Callback):\n",
    "    \"`Callback` that tracks the number of iterations done and properly sets training/eval mode\"\n",
    "    order,run_valid = -10,False\n",
    "\n",
    "    def after_create(self): \n",
    "        self.learn.n_epoch = 1\n",
    "\n",
    "    def before_fit(self):\n",
    "        \"Set the iter and epoch counters to 0, put the model and the right device\"\n",
    "        self.learn.epoch,self.learn.loss = 0,tensor(0.)\n",
    "        self.learn.train_iter,self.learn.pct_train = 0,0.\n",
    "        device = getattr(self.dls, 'device', default_device())\n",
    "        self.model.to(device)\n",
    "        if isinstance(self.loss_func, (nn.Module, BaseLoss)): \n",
    "            self.loss_func.to(device)\n",
    "        if hasattr(self.model, 'reset'): \n",
    "            self.model.reset()\n",
    "\n",
    "    def after_batch(self):\n",
    "        \"Update the iter counter (in training mode)\"\n",
    "        self.learn.pct_train += 1./(self.n_iter*self.n_epoch)\n",
    "        self.learn.train_iter += 1\n",
    "\n",
    "    def before_train(self):\n",
    "        \"Set the model to training mode\"\n",
    "        self.learn.pct_train=self.epoch/self.n_epoch\n",
    "        self.model.train()\n",
    "        self.learn.training=True\n",
    "\n",
    "    def before_validate(self):\n",
    "        \"Set the model to validation mode\"\n",
    "        self.model.eval()\n",
    "        self.learn.training=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MixedPrecision` is an example of modifying the training loop via callback. Here we apply PyTorch's `autocast` and gradient scaling to train models using Automatic Mixed Precision.\n",
    "\n",
    "We can also see a use of `CancelStepException` to make fastai optimziers compatible with `GradScaler`, which expects a PyTorch optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixedPrecision(Callback):\n",
    "    \"Mixed precision training using Pytorch's `autocast` and `GradScaler`\"\n",
    "    order = 10\n",
    "    def __init__(self, **kwargs): \n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def before_fit(self): \n",
    "        self.autocast,self.learn.scaler,self.scales = autocast(),GradScaler(**self.kwargs),L()\n",
    "\n",
    "    def before_batch(self):\n",
    "        self.autocast.__enter__()\n",
    "\n",
    "    def after_pred(self):\n",
    "        if next(flatten(self.pred)).dtype==torch.float16: \n",
    "            self.learn.pred = to_float(self.pred)\n",
    "\n",
    "    def after_loss(self): \n",
    "        self.autocast.__exit__(None, None, None)\n",
    "\n",
    "    def before_backward(self): \n",
    "        self.learn.loss_grad = self.scaler.scale(self.loss_grad)\n",
    "\n",
    "    def before_step(self):\n",
    "        \"Use `self` as a fake optimizer. `self.skipped` will be set to True `after_step` if gradients overflow. \"\n",
    "        self.skipped=True\n",
    "        self.scaler.step(self)\n",
    "        if self.skipped:\n",
    "            raise CancelStepException()\n",
    "        self.scales.append(self.scaler.get_scale())\n",
    "\n",
    "    def after_step(self): \n",
    "        self.learn.scaler.update()\n",
    "\n",
    "    @property \n",
    "    def param_groups(self): \n",
    "        \"Pretend to be an optimizer for `GradScaler`\"\n",
    "        return self.opt.param_groups\n",
    "\n",
    "    def step(self, *args, **kwargs): \n",
    "        \"Fake optimizer step to detect whether this batch was skipped from `GradScaler`\"\n",
    "        self.skipped=False\n",
    "\n",
    "    def after_fit(self): \n",
    "        self.autocast,self.learn.scaler,self.scales = None,None,None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also modify training data via a Callback. Here is how fastai applies MixUp to images and labels (in combination with `MixHandler` to correctly apply the loss):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixUp(MixHandler):\n",
    "    \"Implementation of https://arxiv.org/abs/1710.09412\"\n",
    "    def __init__(self, \n",
    "        alpha:float=.4 # Determine `Beta` distribution in range (0.,inf]\n",
    "    ): \n",
    "        super().__init__(alpha)\n",
    "        \n",
    "    def before_batch(self):\n",
    "        \"Blend xb and yb with another random item in a second batch (xb1,yb1) with `lam` weights\"\n",
    "        lam = self.distrib.sample((self.y.size(0),)).squeeze().to(self.x.device)\n",
    "        lam = torch.stack([lam, 1-lam], 1)\n",
    "        self.lam = lam.max(1)[0]\n",
    "        shuffle = torch.randperm(self.y.size(0)).to(self.x.device)\n",
    "        xb1,self.yb1 = tuple(L(self.xb).itemgot(shuffle)),tuple(L(self.yb).itemgot(shuffle))\n",
    "        nx_dims = len(self.x.size())\n",
    "        self.learn.xb = tuple(L(xb1,self.xb).map_zip(torch.lerp,weight=unsqueeze(self.lam, n=nx_dims-1)))\n",
    "\n",
    "        if not self.stack_y:\n",
    "            ny_dims = len(self.y.size())\n",
    "            self.learn.yb = tuple(L(self.yb1,self.yb).map_zip(torch.lerp,weight=unsqueeze(self.lam, n=ny_dims-1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('fastai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13823a492421f5f84f9059b75c88ebb0eec72237eae812caa6d0bf5be399ae62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
