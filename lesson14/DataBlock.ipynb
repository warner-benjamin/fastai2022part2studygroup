{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastai provides the `DataBlock` api, which is a high (or mid) level api to easily create DataSets and DataLoaders out of composable pieces.\n",
    "\n",
    "Here is an example of creating a train and valid DataLoader from the DataBlock api on Imagenette with both CPU and GPU augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenette_stats = ([0.465,0.458,0.429],[0.285,0.28,0.301])\n",
    "\n",
    "def get_dls(size, bs, augs=None, workers=None):\n",
    "    path = URLs.IMAGENETTE_320\n",
    "    source = untar_data(path)\n",
    "\n",
    "    if workers is None: \n",
    "        workers = min(8, num_cpus())\n",
    "\n",
    "    batch_tfms = [Normalize.from_stats(*imagenette_stats)]\n",
    "    if augs: \n",
    "        batch_tfms += augs\n",
    "\n",
    "    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                       splitter=GrandparentSplitter(valid_name='val'),\n",
    "                       get_items=get_image_files, \n",
    "                       get_y=parent_label,\n",
    "                       item_tfms=[RandomResizedCrop(size, min_scale=0.35), FlipItem(0.5)],\n",
    "                       batch_tfms=batch_tfms)\n",
    "\n",
    "    return dblock.dataloaders(source, path=source, bs=bs, num_workers=workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataBlock api has many items which should be familiar from the PyTorch DataLoaders example in the lesson:\n",
    "\n",
    "- bs: the batch size\n",
    "- num_workers: create a multiprocessing DataLoader\n",
    "\n",
    "It also has items which are unfamiliar. Which we will go over."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blocks\n",
    "\n",
    "First is the `blocks` in `DataBlock`. These are precreated, mix and matchable, methods to ingest data into a format which the PyTorch model expects.\n",
    "\n",
    "`ImageBlock` says our training data is images and `CategoryBlock` says our labels are categories.\n",
    "\n",
    "If we had more than one input, say both RGB and Greyscale images, we would pass in three blocks and set `n_inp=2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dblock = DataBlock(blocks=(ImageBlock, ImageBlock(PILImageBW), CategoryBlock),\n",
    "                   splitter=GrandparentSplitter(valid_name='val'),\n",
    "                   get_items=get_image_files, \n",
    "                   get_y=parent_label,\n",
    "                   n_inp=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ImageBlock` tells the `DataBlock` api to read in `PILImage`s and converts the uint8 format to float tensors on the GPU thanks to passing `IntToFloatTensor` to `TransformBlock`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImageBlock(cls:PILBase=PILImage):\n",
    "    \"A `TransformBlock` for images of `cls`\"\n",
    "    return TransformBlock(type_tfms=cls.create, batch_tfms=IntToFloatTensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PILImage` can read in images from multiple sources, and knows how to plot itself thanks to `show` and `show_image`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PILBase(Image.Image, metaclass=BypassNewMeta):\n",
    "    _bypass_type=Image.Image\n",
    "    _show_args = {'cmap':'viridis'}\n",
    "    _open_args = {'mode': 'RGB'}\n",
    "\n",
    "    @classmethod\n",
    "    def create(cls, fn:Path|str|Tensor|ndarray|bytes, **kwargs)->None:\n",
    "        \"Open an `Image` from path `fn`\"\n",
    "        if isinstance(fn,TensorImage): fn = fn.permute(1,2,0).type(torch.uint8)\n",
    "        if isinstance(fn, TensorMask): fn = fn.type(torch.uint8)\n",
    "        if isinstance(fn,Tensor): fn = fn.numpy()\n",
    "        if isinstance(fn,ndarray): return cls(Image.fromarray(fn))\n",
    "        if isinstance(fn,bytes): fn = io.BytesIO(fn)\n",
    "        return cls(load_image(fn, **merge(cls._open_args, kwargs)))\n",
    "\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        \"Show image using `merge(self._show_args, kwargs)`\"\n",
    "        return show_image(self, ctx=ctx, **merge(self._show_args, kwargs))\n",
    "\n",
    "class PILImage(PILBase): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PILImageBW` tells pillow to read images in greyscale and passes the greyscale `cmap` arg to `show_image`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PILImageBW(PILImage): \n",
    "    _show_args = {'cmap':'Greys'}\n",
    "    _open_args = {'mode': 'L'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost all transforms in fastai use TypeDispatch to apply to the correct types and ignore types which they are not supposed to modify.\n",
    "\n",
    "Here we are monkey patching `ToTensor` to use TypeDispatch to convert a `PILBase` image to a Tensor. Specifically, `TensorImage`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image2tensor(img):\n",
    "    \"Transform image to byte tensor in `c*h*w` dim order.\"\n",
    "    res = tensor(img)\n",
    "    if res.dim()==2: res = res.unsqueeze(-1)\n",
    "    return res.permute(2,0,1)\n",
    "\n",
    "PILImage._tensor_cls = TensorImage\n",
    "\n",
    "@ToTensor\n",
    "def encodes(self, o:PILBase):\n",
    "    return o._tensor_cls(image2tensor(o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that `IntToFloatTensor` knows how to correctly encode and decode a `TensorImage` from uint8 to float for passing to the model and plotting, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntToFloatTensor(DisplayedTransform):\n",
    "    \"Transform image to float tensor, optionally dividing by 255 (e.g. for images).\"\n",
    "    order = 10 # Need to run after PIL transforms on the GPU\n",
    "    def __init__(self, div=255., div_mask=1): \n",
    "        store_attr()\n",
    "\n",
    "    def encodes(self, o:TensorImage): \n",
    "        return o.float().div_(self.div)\n",
    "\n",
    "    def decodes(self, o:TensorImage): \n",
    "        return ((o.clamp(0., 1.) * self.div).long()) if self.div else o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, `CategoryBlock` can read in labels from multiple sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CategoryBlock(\n",
    "    vocab:list|pd.Series=None, # List of unique class names\n",
    "    sort:bool=True, # Sort the classes alphabetically\n",
    "    add_na:bool=False, # Add `#na#` to `vocab`\n",
    "):\n",
    "    \"`TransformBlock` for single-label categorical targets\"\n",
    "    return TransformBlock(type_tfms=Categorize(vocab=vocab, sort=sort, add_na=add_na))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And by passing `Categorize` to `TransformBlock`, it will automatically generate a vocab (if needed), and contains methods which `encodes` the text labels to integers and `decodes` them back to text when we plot or display results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Categorize(DisplayedTransform):\n",
    "    \"Reversible transform of category string to `vocab` id\"\n",
    "    loss_func,order=CrossEntropyLossFlat(),1\n",
    "    def __init__(self, vocab=None, sort=True, add_na=False):\n",
    "        if vocab is not None: \n",
    "            vocab = CategoryMap(vocab, sort=sort, add_na=add_na)\n",
    "        store_attr()\n",
    "\n",
    "    def setups(self, dsets):\n",
    "        if self.vocab is None and dsets is not None: \n",
    "            self.vocab = CategoryMap(dsets, sort=self.sort, add_na=self.add_na)\n",
    "        self.c = len(self.vocab)\n",
    "\n",
    "    def encodes(self, o): \n",
    "        try:\n",
    "            return TensorCategory(self.vocab.o2i[o])\n",
    "        except KeyError as e:\n",
    "            raise KeyError(f\"Label '{o}' was not included in the training dataset\") from e\n",
    "\n",
    "    def decodes(self, o): \n",
    "        return Category(self.vocab[o])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple built in blocks in the DataBlock api. `MultiCategoryBlock` and `RegressionBlock` for labels. Along with more task specific blocks, which you should look up in the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitters\n",
    "\n",
    "Next in the datablock is the splitter, which splits the data into the train and validation set.\n",
    "\n",
    "Here we are using a `GrandparentSplitter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dls(size, bs, augs=None, workers=None):\n",
    "    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                        splitter=GrandparentSplitter(valid_name='val'),\n",
    "                        get_items=get_image_files, \n",
    "                        get_y=parent_label,\n",
    "                        item_tfms=[RandomResizedCrop(size, min_scale=0.35), FlipItem(0.5)],\n",
    "                        batch_tfms=batch_tfms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GrandparentSplitter` splits the data based on a folder name.\n",
    "\n",
    "There are a bunch of splitters defined in fastai. For example, `ColSplitter` for splitting via a pandas `DataFrame`, `IndexSplitter` by index, etc. See [https://docs.fast.ai/data.transforms.html#split](https://docs.fast.ai/data.transforms.html#split) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to let the datablock know where the items are. Which we accomplish via `get_items`, `get_x`, and `get_y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dls(size, bs, source, batch_tfms=None, workers=None):\n",
    "    dblock = DataBlock(blocks=(ImageBlock, CategoryBlock),\n",
    "                        splitter=GrandparentSplitter(valid_name='val'),\n",
    "                        get_items=get_image_files, \n",
    "                        get_y=parent_label,\n",
    "                        item_tfms=[RandomResizedCrop(size, min_scale=0.35), FlipItem(0.5)],\n",
    "                        batch_tfms=batch_tfms)\n",
    "\n",
    "    dblock.dataloaders(source, path=source, bs=bs, num_workers=workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_image_files` knows to read images from the path we pass into the dataloader. `parent_label` reads the folder name as a label. See [https://docs.fast.ai/data.transforms.html](https://docs.fast.ai/data.transforms.html) for more item getters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fastai has a large collection of built in transformations for Images.\n",
    "\n",
    "They have five primary features.\n",
    "\n",
    "1. Transforms have an `order` to correctly sort themselves\n",
    "2. Transforms use TypeDispatch to apply on the correct data types\n",
    "3. Transforms know how to undo themselves for plotting \n",
    "4. Transforms have both random train and non-random valid implemetentations in one definition\n",
    "5. GPU Transformations smartly combine Affine transforms and Lighting transforms into one step to save computation\n",
    "\n",
    "We can see in `RandomCrop` that it uses `split_idx` to apply a constant center crop if this is the validation set, or a random crop if the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomCrop(RandTransform):\n",
    "    \"Randomly crop an image to `size`\"\n",
    "    split_idx = None\n",
    "    order = 1\n",
    "\n",
    "    def __init__(self, \n",
    "        size:int|tuple, # Size to crop to, duplicated if one value is specified\n",
    "        **kwargs\n",
    "    ):\n",
    "        size = _process_sz(size)\n",
    "        store_attr()\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def before_call(self, \n",
    "        b, \n",
    "        split_idx:int # Index of the train/valid dataset\n",
    "    ):\n",
    "        \"Randomly positioning crop if train dataset else center crop\"\n",
    "        self.orig_sz = _get_sz(b)\n",
    "        if split_idx: \n",
    "            self.tl = (self.orig_sz-self.size)//2\n",
    "        else:\n",
    "            wd = self.orig_sz[0] - self.size[0]\n",
    "            hd = self.orig_sz[1] - self.size[1]\n",
    "            w_rand = (wd, -1) if wd < 0 else (0, wd)\n",
    "            h_rand = (hd, -1) if hd < 0 else (0, hd)\n",
    "            self.tl = fastuple(random.randint(*w_rand), random.randint(*h_rand))\n",
    "\n",
    "    def encodes(self, x:Image.Image|TensorBBox|TensorPoint):\n",
    "        return x.crop_pad(self.size, self.tl, orig_sz=self.orig_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And `Normalize` has both `encodes` and `decodes` methods which use TypeDispatch to normalize images before passing to the model or unnormalize them when we plot or display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(DisplayedTransform):\n",
    "    \"Normalize/denorm batch of `TensorImage`\"\n",
    "    parameters = L('mean', 'std')\n",
    "    order = 99\n",
    "    def __init__(self, mean=None, std=None, axes=(0,2,3)): \n",
    "        store_attr()\n",
    "\n",
    "    @classmethod\n",
    "    def from_stats(cls, mean, std, dim=1, ndim=4, cuda=True): \n",
    "        return cls(*broadcast_vec(dim, ndim, mean, std, cuda=cuda))\n",
    "\n",
    "    def setups(self, dl:DataLoader):\n",
    "        if self.mean is None or self.std is None:\n",
    "            x,*_ = dl.one_batch()\n",
    "            self.mean,self.std = x.mean(self.axes, keepdim=True),x.std(self.axes, keepdim=True)+1e-7\n",
    "\n",
    "    def encodes(self, x:TensorImage): \n",
    "        return (x-self.mean) / self.std\n",
    "\n",
    "    def decodes(self, x:TensorImage):\n",
    "        f = to_cpu if x.device.type=='cpu' else noop\n",
    "        return (x*f(self.std) + f(self.mean))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('fastai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13823a492421f5f84f9059b75c88ebb0eec72237eae812caa6d0bf5be399ae62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
