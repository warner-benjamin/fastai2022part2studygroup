{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from collections.abc import MutableSequence\n",
    "\n",
    "from fastai.vision.all import *\n",
    "from fastai.optimizer import _BaseOptimizer, _update\n",
    "\n",
    "from fastxtend.optimizer.foreach import ForEachOptimizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers: Part 2\n",
    "\n",
    "In the Lesson 14 Optimizers notebook, we went over PyTorch and fastai Optimizers and showed how SGD was implemented in both frameworks.\n",
    "\n",
    "In this notebook, we will review how fastai Optimizers work and implement AdamW from the paper's algorithm.\n",
    "\n",
    "## Fastai Optimizer Review\n",
    "\n",
    "fastai optimizers are unique in two ways. First, fastai Optimizers can set weight decay (and any hyperparameter) uniquely per parameter. This allows fastai Optimizers to have half as many parameter groups as PyTorch Optimizers, which simplifies discriminative learning rates out of the box. i.e. different learning rates per parameter group. \n",
    "\n",
    "Second, fastai Optimizers are defined as a series of optim step callbacks, which are called by `Optimizer` one after another. Which allows fastai to reuse optimizer steps across multiple optimizers.\n",
    "\n",
    "Like PyTorch optimizers, `Optimizer` is initialized with the parameters we want to train and any optimizer specific hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(_BaseOptimizer):\n",
    "    \"Base optimizer class for the fastai library, updating `params` with `cbs`\"\n",
    "    _keep_on_clear = ['force_train', 'do_wd']\n",
    "    def __init__(self,\n",
    "        params:Tensor|Iterable, # Model parameters\n",
    "        cbs:callable|MutableSequence, # `Optimizer` step callbacks\n",
    "        **defaults # Hyper parameters default values\n",
    "    ):\n",
    "        params = L(params)\n",
    "        self.cbs,self.state = L(cbs),defaultdict(dict)\n",
    "        defaults = merge(*self.cbs.attrgot('defaults'), defaults)\n",
    "        self.param_lists = L(L(p) for p in params) if isinstance(params[0], (L,list)) else L([params])\n",
    "        self.hypers = L({} for _ in range_of(self.param_lists))\n",
    "        self.set_hypers(**defaults)\n",
    "        self.frozen_idx = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Optimizer step function loops through all parameter groups and collect parameter groups default values, and then loops through all parameters and applies the optimization step to any parameter with a gradient.\n",
    "\n",
    "There is an additional loop for optimizer callbacks, which apply the optimizer steps sequentially and update the per parameter state dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_params(self,\n",
    "    with_grad:bool=False # Get all parameters. If `True` select only those with a gradient\n",
    "):\n",
    "    res = L((p,pg,self.state[p],hyper) for pg,hyper in zip(self.param_lists,self.hypers) for p in pg)\n",
    "    return L(o for o in res if hasattr(o[0], 'grad') and o[0].grad is not None) if with_grad else res\n",
    "\n",
    "\n",
    "def step(self):\n",
    "    for p,pg,state,hyper in self.all_params(with_grad=True):\n",
    "        for cb in self.cbs: \n",
    "            state = _update(state, cb(p, **{**state, **hyper}))\n",
    "        self.state[p] = state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simplified SGDW (without momentum) shows how we would define a fastai optimizer out of separate Optimizer callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_decay(p, lr, wd, do_wd=True, **kwargs):\n",
    "    \"Weight decay as decaying `p` with `lr*wd`\"\n",
    "    if do_wd and wd!=0: \n",
    "        p.data.mul_(1 - lr*wd)\n",
    "\n",
    "def sgd_step(p, lr, **kwargs):\n",
    "    \"Step for SGD with `lr`\"\n",
    "    p.data.add_(p.grad.data, alpha=-lr)\n",
    "\n",
    "def SGD(\n",
    "    params:Tensor|Iterable, # Model parameters\n",
    "    lr:float|slice, # Default learning rate\n",
    "    wd:float=0., # Optional true weight decay\n",
    ") -> Optimizer:\n",
    "    \"A SGD `Optimizer`\"\n",
    "    cbs = [weight_decay, sgd_step]\n",
    "    return Optimizer(params, cbs, lr=lr, wd=wd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing AdamW from Scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdamW was introduced as an improvement of the Adam Optimizer in [Decoupled Weight Decay Regularization](https://arxiv.org/abs/1711.05101) by Ilya Loshchilov & Frank Hutter.\n",
    "\n",
    "It introduces decoupled (or true) weight decay, which unlike L2 regularization, does not directly interact with the learning rate, allowing weight decay and the learning rate to be tuned independently. AdamW's weight decay also outperforms L2 regularization on most tasks. (And thus is the default Adam in fastai)\n",
    "\n",
    "AdamW is defined by the following formula:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "t \\leftarrow t + 1 \\\\[0.5em]\n",
    "\\bm{m}_t \\leftarrow \\beta_1 \\bm{m}_{t-1} + (1 - \\beta_1) \\bm{g}_t \\\\[0.5em]\n",
    "\\bm{v}_t \\leftarrow \\beta_2 \\bm{v}_{t-1} + (1 - \\beta_2) \\bm{g}^2_t \\\\[0.5em]\n",
    "\\hat{\\bm{m}}_t \\leftarrow \\bm{m}_t/(1 - \\beta_1^t) \\\\[0.5em]\n",
    "\\hat{\\bm{{v}}}_t \\leftarrow \\bm{v}_t/(1 - \\beta_2^t) \\\\[0.5em]\n",
    "\\eta_t \\leftarrow \\text{SetScheduleMultiplier}(t) \\\\[0.5em]\n",
    "\\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\eta_t \\left( \\hat{\\bm{m}}_t / (\\sqrt{\\hat{\\bm{v}}_t} + \\epsilon) + \\lambda\\bm{\\theta}_{t-1} \\right) \\\\[0.5em]\n",
    "\\end{aligned}\n",
    "$$\n",
    "which we will go through and implement line by line.\n",
    "\n",
    "First, we assume we have some parameter `param` and a gradient of that parameter `grad` passed to our AdamW optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = Tensor(0)\n",
    "grad = Tensor(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line simply keeps track of the number of steps we've taken, incrementing it by one every pass\n",
    "$$t \\leftarrow t + 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = 0\n",
    "\n",
    "step += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, assuming we have a gradient from a PyTorch model, we calculate the moving average $\\bm{m}_t$ of the gradient $\\bm{g}_t$. \n",
    "$$\\bm{m}_t \\leftarrow \\beta_1 \\bm{m}_{t-1} + (1 - \\beta_1) \\bm{g}_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_avg = None\n",
    "mom = 0.9\n",
    "\n",
    "if grad_avg is None: \n",
    "    grad_avg = torch.zeros_like(param, memory_format=torch.preserve_format)\n",
    "\n",
    "# average_grad\n",
    "grad_avg = grad_avg.mul(mom).add(grad, alpha=1-mom)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we calculate the squared moving average $\\bm{v}_t$ of the gradient $\\bm{g}_t$. \n",
    "$$\\bm{v}_t \\leftarrow \\beta_2 \\bm{v}_{t-1} + (1 - \\beta_2) \\bm{g}^2_t$$\n",
    "where `addcmul` is a single op to add the weighted multiplied terms to the first term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqr_avg = None\n",
    "sqr_mom = 0.99\n",
    "\n",
    "if sqr_avg is None: \n",
    "    sqr_avg  = torch.zeros_like(param, memory_format=torch.preserve_format)\n",
    "\n",
    "# average_sqr_grad\n",
    "sqr_avg = sqr_avg.mul(sqr_mom).addcmul(grad, grad, value=1-sqr_mom)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we calculate the debias terms and debias our gradient moving average and gradient squared moving average\n",
    "$$\n",
    "\\hat{\\bm{m}}_t \\leftarrow \\bm{m}_t/(1 - \\beta_1^t) \\\\[0.5em]\n",
    "\\hat{\\bm{{v}}}_t \\leftarrow \\bm{v}_t/(1 - \\beta_2^t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debias1 = 1-mom**step\n",
    "debias2 = 1-sqr_mom**step\n",
    "\n",
    "grad_avg_d = grad_avg/debias1\n",
    "sqr_avg_d  = sqr_avg /debias2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next line in the equation tells us that the Learning Rate has been set by a scheduler\n",
    "$$\\eta_t \\leftarrow \\text{SetScheduleMultiplier}(t)$$\n",
    "And finally we arrive to the AdamW step.\n",
    "$$\\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\eta_t \\left( \\hat{\\bm{m}}_t / (\\sqrt{\\hat{\\bm{v}}_t} + \\epsilon) + \\lambda\\bm{\\theta}_{t-1} \\right)$$\n",
    "This is a lot in one equation, so we'll break it down into multiple parts.\n",
    "\n",
    "First lets look at the decoupled (true) weight decay all the way on the right.\n",
    "$$\\lambda\\bm{\\theta}_{t-1}$$\n",
    "The full equation applied to the parameter weights includes the negative learning rate, so we add that to our code\n",
    "$$\\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\eta_t\\lambda\\bm{\\theta}_{t-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 3e-3\n",
    "wd = 1e-2\n",
    "\n",
    "param = param.mul(1 - lr*wd)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we calculate the actual weight update from our debiased moving averages, taking the square root of the squared moving average, adding an epsilon term for mathematical stability, and dividing the moving average by the square root of the squared moving average:\n",
    "$$\\bm{u}_t = \\hat{\\bm{m}}_t / (\\sqrt{\\hat{\\bm{v}}_t} + \\epsilon)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-5\n",
    "\n",
    "update = grad_avg_d/(torch.sqrt(sqr_avg_d) + eps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we update the parameter by our Adam step\n",
    "$$\\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\eta_t \\bm{u}_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = torch.add(param, update, value =-lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in practice, this is slow, so we combine the entire update step into as few operations as possible using `addcdiv`:\n",
    "$$\\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\eta_t \\left( \\hat{\\bm{m}}_t / (\\sqrt{\\hat{\\bm{v}}_t} + \\epsilon) \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = torch.addcdiv(param, grad_avg_d, torch.sqrt(sqr_avg_d) + eps, value = -lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there we have it, the entire AdamW Optimizer update algorithm\n",
    "$$\n",
    "\\begin{aligned}\n",
    "t \\leftarrow t + 1 \\\\[0.5em]\n",
    "\\bm{m}_t \\leftarrow \\beta_1 \\bm{m}_{t-1} + (1 - \\beta_1) \\bm{g}_t \\\\[0.5em]\n",
    "\\bm{v}_t \\leftarrow \\beta_2 \\bm{v}_{t-1} + (1 - \\beta_2) \\bm{g}^2_t \\\\[0.5em]\n",
    "\\hat{\\bm{m}}_t \\leftarrow \\bm{m}_t/(1 - \\beta_1^t) \\\\[0.5em]\n",
    "\\hat{\\bm{{v}}}_t \\leftarrow \\bm{v}_t/(1 - \\beta_2^t) \\\\[0.5em]\n",
    "\\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\eta_t \\left( \\hat{\\bm{m}}_t / (\\sqrt{\\hat{\\bm{v}}_t} + \\epsilon) + \\lambda\\bm{\\theta}_{t-1} \\right) \\\\[0.5em]\n",
    "\\end{aligned}\n",
    "$$\n",
    "in one fastai optimizer compatible method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_step(param:Tensor, grad:Tensor, lr:float, wd:float, mom:float, sqr_mom:float, eps:float, \n",
    "              grad_avg:Optional[Tensor]=None, sqr_avg:Optional[Tensor]=None, step:int=0):\n",
    "    if grad_avg is None: \n",
    "        grad_avg = torch.zeros_like(param, memory_format=torch.preserve_format)\n",
    "    if sqr_avg is None: \n",
    "        sqr_avg  = torch.zeros_like(param, memory_format=torch.preserve_format)\n",
    "\n",
    "    step += 1\n",
    "    if wd != 0:\n",
    "        # true weight_decay\n",
    "        param = param.mul(1 - lr*wd)\n",
    "\n",
    "    # average_grad\n",
    "    grad_avg = grad_avg.mul(mom).add(grad, alpha=1-mom)\n",
    "\n",
    "    # average_sqr_grad\n",
    "    sqr_avg = sqr_avg.mul(sqr_mom).addcmul(grad, grad, value=1-sqr_mom)\n",
    "\n",
    "    # adam_step\n",
    "    debias1 = 1-mom**step\n",
    "    debias2 = 1-sqr_mom**step\n",
    "    param = torch.addcdiv(param, grad_avg, torch.sqrt(sqr_avg/debias2) + eps, value = -lr / debias1)\n",
    "\n",
    "    return {'grad_avg': grad_avg, 'sqr_avg': sqr_avg, 'step': step}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can We Go Faster?\n",
    "\n",
    "The only problem with this formulation of AdamW, and the fastai callbacks version, is it's slow. Why is that? Because PyTorch must call all of these operations on every Tensor in our model sequentially. Which means we are relaunching the same `mul`, `add`, `addcmul`, `addcdiv`, etc Cuda kernels multiple times.\n",
    "\n",
    "Is there a way to get around this and apply the optimizer step even faster? \n",
    "\n",
    "Yes. Two actually.\n",
    "\n",
    "The first way is to use PyTorch's undocumented `foreach` methods. Unlike `torch.mul` which applys a multiply operation on one Tensor at a time, `torch._foreach_mul` takes a list of Tensors and applies the multiply operation on them in parallel with as few Cuda kernel launches as possible (I assume, I haven't looked at the Cuda code for these methods).\n",
    "\n",
    "So let’s rewrite AdamW, except using the faster `foreach` methods.\n",
    "\n",
    "First, we need an `Optimizer` that loops through our model's parameters, grabs all the Tensors with gradients, and adds them to lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|exporti\n",
    "class AdamForEachOptimizer(ForEachOptimizer):\n",
    "    \"An `ForEachOptimizer` with a modified step for `adam_foreach_step`\"\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        if closure is not None: raise NotImplementedError(\"fastai optimizers currently do not support closure\")\n",
    "        for pg, hyper in zip(self.param_lists, self.hypers):\n",
    "            pl, gl, grad_avg, sqr_avg, ones, steps, do_wd = [], [], [], [], [], [], []\n",
    "\n",
    "            for p in pg:\n",
    "                if hasattr(p, 'grad') and p.grad is not None:\n",
    "                    state = self.state[p]\n",
    "\n",
    "                    if 'step' not in state:\n",
    "                        state['grad_avg'] = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        state['sqr_avg']  = torch.zeros_like(p, memory_format=torch.preserve_format)\n",
    "                        state['step'] = 0\n",
    "\n",
    "                    state['step'] += 1\n",
    "                    pl.append(p)\n",
    "                    gl.append(p.grad)\n",
    "                    grad_avg.append(state['grad_avg'])\n",
    "                    sqr_avg.append(state['sqr_avg'])\n",
    "                    steps.append(state['step'])\n",
    "                    do_wd.append(state.get('do_wd', True))\n",
    "\n",
    "            self.opt_step(param=pl, grad=gl, grad_avg=grad_avg, sqr_avg=sqr_avg,\n",
    "                          steps=np.array(steps, dtype=np.int32), do_wd=np.array(do_wd, dtype=bool), \n",
    "                          decouple_wd=self.decouple_wd, **hyper)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to keep track of all the steps in the AdamW step implementation, since we are already doing that in `AdamForEachOptimizer.step`\n",
    "```\n",
    "if 'step' not in state:\n",
    "    state['step'] = 0\n",
    "\n",
    "state['step'] += 1\n",
    "```\n",
    "so we can jump straight to the gradient $\\bm{g}_t$ moving average $\\bm{m}_t$ .\n",
    "$$\\bm{m}_t \\leftarrow \\beta_1 \\bm{m}_{t-1} + (1 - \\beta_1) \\bm{g}_t$$\n",
    "The `torch._foreach_mul_` method takes an existing list of Tensors, `grad_avg`, and applies the operation `mul` on all of the Tensors, just like `torch.mul_(grad_avg, mom)` would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch._foreach_mul_(grad_avg, mom)\n",
    "torch._foreach_add_(grad_avg, grad, alpha=1-mom)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we calculate the squared moving average $\\bm{v}_t$ of the gradient $\\bm{g}_t$. \n",
    "$$\\bm{v}_t \\leftarrow \\beta_2 \\bm{v}_{t-1} + (1 - \\beta_2) \\bm{g}^2_t$$\n",
    "where `addcmul` is a single op to add the weighted multiplied terms to the first term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch._foreach_mul_(sqr_avg, sqr_mom)\n",
    "torch._foreach_addcmul_(sqr_avg, grad, grad, value=1-sqr_mom)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we calculate the debias terms and debias our gradient moving average and gradient squared moving average\n",
    "$$\n",
    "\\hat{\\bm{m}}_t \\leftarrow \\bm{m}_t/(1 - \\beta_1^t) \\\\[0.5em]\n",
    "\\hat{\\bm{{v}}}_t \\leftarrow \\bm{v}_t/(1 - \\beta_2^t)\n",
    "$$\n",
    "In this case, steps are a NumPy array, so all the debias terms (which might be different per Tensor) can be vectorized. But we won't apply them to `grad_avg` and `sqr_avg` until later. \n",
    "\n",
    "Note, we include the learning rate in the first debias term and take the square root of the squared moving average debias term here, so we don't have to repeat ops later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = np.array([1,1,1,...])\n",
    "\n",
    "debias1 = -lr / (1 - mom**steps)\n",
    "debias2 = np.sqrt(1 - sqr_mom**steps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like our slow implementation, we will break up the AdamW step into multiple substeps\n",
    "$$\\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\eta_t \\left( \\hat{\\bm{m}}_t / (\\sqrt{\\hat{\\bm{v}}_t} + \\epsilon) + \\lambda\\bm{\\theta}_{t-1} \\right)$$\n",
    "First being the weight decay term\n",
    "$$\\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\eta_t\\lambda\\bm{\\theta}_{t-1}$$\n",
    "Not all parameters necessarily apply weight decay, so I use the `np.where` to calculate weight decay for those parameters and multiply by 1 for the parameters without weight decay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_wd = np.array([True, False, True, ...])\n",
    "\n",
    "if wd != 0:\n",
    "    # weight_decay\n",
    "    wd = np.where(do_wd, 1-lr*wd, 1.)\n",
    "    torch._foreach_mul_(param, scalars=wd.tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we calculate the debiased root of the squared moving average\n",
    "$$\\bm{r}_{t} = \\sqrt{\\hat{\\bm{v}}_t} + \\epsilon$$\n",
    "due to limitations in the `foreach` methods, this becomes three lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqr_avg_debias2 = torch._foreach_sqrt(sqr_avg)\n",
    "torch._foreach_div_(sqr_avg_debias2, debias2.tolist())\n",
    "torch._foreach_add_(sqr_avg_debias2, eps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we finally calculate the rest of the Adam step\n",
    "$$\\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\eta_t \\left( \\hat{\\bm{m}}_t / \\bm{r}_{t} \\right)$$\n",
    "where `debias1` term has the negative learning rate included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch._foreach_addcdiv_(param, grad_avg, sqr_avg_debias2, scalars=debias1.tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we reimplemented AdamW in PyTorch again.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\bm{m}_t \\leftarrow \\beta_1 \\bm{m}_{t-1} + (1 - \\beta_1) \\bm{g}_t \\\\[0.5em]\n",
    "\\bm{v}_t \\leftarrow \\beta_2 \\bm{v}_{t-1} + (1 - \\beta_2) \\bm{g}^2_t \\\\[0.5em]\n",
    "\\hat{\\bm{m}}_t \\leftarrow \\bm{m}_t/(1 - \\beta_1^t) \\\\[0.5em]\n",
    "\\hat{\\bm{{v}}}_t \\leftarrow \\bm{v}_t/(1 - \\beta_2^t) \\\\[0.5em]\n",
    "\\bm{\\theta}_t \\leftarrow \\bm{\\theta}_{t-1} - \\eta_t \\left( \\hat{\\bm{m}}_t / (\\sqrt{\\hat{\\bm{v}}_t} + \\epsilon) + \\lambda\\bm{\\theta}_{t-1} \\right) \\\\[0.5em]\n",
    "\\end{aligned}\n",
    "$$\n",
    "Except it runs 21 to 293 percent faster than our first implementation (across models I tested). You can find this, and a handful of other fastai optimizers reimplemented using foreach methods in [fastxtend](https://fastxtend.benjaminwarner.dev/optimizer.fused.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_foreach_step(p:list[Tensor], g:list[Tensor], grad_avg:list[Tensor], sqr_avg:list[Tensor], ones:list[Tensor|None], \n",
    "                      steps:np.ndarray[Any, int], do_wd:np.ndarray[Any, bool], lr:float, wd:float, mom:float, sqr_mom:float, \n",
    "                      eps:float, **kwargs):\n",
    "    if wd != 0:\n",
    "        # weight_decay\n",
    "        wd = np.where(do_wd, 1-lr*wd, 1.)\n",
    "        torch._foreach_mul_(p, scalars=wd.tolist())\n",
    "\n",
    "    # average_grad, dampening=True\n",
    "    torch._foreach_mul_(grad_avg, mom)\n",
    "    torch._foreach_add_(grad_avg, g, alpha=1-mom)\n",
    "\n",
    "    # average_sqr_grad\n",
    "    torch._foreach_mul_(sqr_avg, sqr_mom)\n",
    "    torch._foreach_addcmul_(sqr_avg, g, g, value=1-sqr_mom)\n",
    "\n",
    "    # adam_step\n",
    "    debias1 = -lr / (1 - mom**steps)\n",
    "    debias2 = np.sqrt(1 - sqr_mom**steps)\n",
    "\n",
    "    sqr_avg_debias2 = torch._foreach_sqrt(sqr_avg)\n",
    "    torch._foreach_div_(sqr_avg_debias2, debias2.tolist())\n",
    "    torch._foreach_add_(sqr_avg_debias2, eps)\n",
    "\n",
    "    torch._foreach_addcdiv_(p, grad_avg, sqr_avg_debias2, debias1.tolist())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use them, simply import `fastxtend.optimizer` pass to `opt_func` like normal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "from fastxtend.optimizer.all import *\n",
    "\n",
    "Learner(..., opt_func=adam(foreach=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But Can We Still Go Faster?\n",
    "Yes. The foreach methods above are vertically fused operations, where we sequentually apply multiple operations to multiple tensors at once.\n",
    "\n",
    "But we can both horizontally and vertically fuse operations to apply multiple steps to multiple tensors at once, instead of sequentially.\n",
    "\n",
    "[Nvidia's Apex](https://github.com/NVIDIA/apex) package does this. It is custom Cuda code, so we won't look at it today. But it's even faster. But not 100% fastai compatible due to not supporting per-parameter weight decay."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:45:29) \n[GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13823a492421f5f84f9059b75c88ebb0eec72237eae812caa6d0bf5be399ae62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
