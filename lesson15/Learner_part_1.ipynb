{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner: Part 1\n",
    "\n",
    "In the past couple lessons, we've been working with a simple training loop like the one below. While it works, we must rewrite portions of it every time we want to change something with our training. And in addition to this annoyance, each time we add or delete something there is a potential for introducing bugs or errors. Some of these bugs can be quite insidious as they don't prevent training, but rather hinder model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb,yb in train_dl:\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tot_loss,tot_acc,count = 0.,0.,0\n",
    "            for xb,yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                n = len(xb)\n",
    "                count += n\n",
    "                tot_loss += loss_func(pred,yb).item()*n\n",
    "                tot_acc  += accuracy (pred,yb).item()*n\n",
    "        print(epoch, tot_loss/count, tot_acc/count)\n",
    "    return tot_loss/count, tot_acc/count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve this in the course we are building MiniAI, a (minimal) flexible framework with callbacks.\n",
    "\n",
    "In practice, we'd use other frameworks with callback and/or extension support like Composer, Lightning, or fastai. Or use a framework to handle the minimal training loop modifications for us like Accelerate.\n",
    "\n",
    "In this notebook we'll look at the fastai Learner, focusing on how it adds, removes, and calls callbacks. And how fastai defines the training loop.\n",
    "\n",
    "## Reviewing Callbacks\n",
    "\n",
    "The lesson14 folder has a notebook on how Fastai defines callbacks, which might be useful to look at before continuing so the material is fresh.\n",
    "\n",
    "## Adding and Removing Callbacks\n",
    "\n",
    "fastai's `Learner` defines two methods for adding callbacks. `add_cb` and `add_cbs`, the latter which is a convenience method adding multiple callbacks using `add_cb`.\n",
    "\n",
    "In this simplified `__init__` method, we can see that `Learner` adds default callbacks and the Callbacks we pass to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(GetAttr):\n",
    "    _default='model'\n",
    "    def __init__(self,\n",
    "        cbs:Callback|list|None=None, # `Callback`s to add to `Learner`\n",
    "        default_cbs:bool=True # Include default callbacks\n",
    "    ):\n",
    "        store_attr(but='dls,cbs')\n",
    "        self.cbs = L()\n",
    "        if default_cbs: \n",
    "            self.add_cbs(L(defaults.callbacks))\n",
    "        self.add_cbs(cbs)\n",
    "        self(\"after_create\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`add_cbs` loops through all callbacks and calls `add_cb`.\n",
    "\n",
    "`add_cb` initializes the callback if needed, set's the `Callback.learn` attribute, sets the callback as an attribute of `Learner`, and then adds the callback to the list of all callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cbs(self, cbs):\n",
    "    L(cbs).map(self.add_cb)\n",
    "    return self\n",
    "\n",
    "def add_cb(self, cb):\n",
    "    if isinstance(cb, type): \n",
    "        cb = cb()\n",
    "    cb.learn = self\n",
    "    setattr(self, cb.name, cb)\n",
    "    self.cbs.append(cb)\n",
    "    return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`remove_cbs` and `remove_cb` does the inverse. Which some special handeling using `_grab_cbs` if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_cbs(self, cbs):\n",
    "    L(cbs).map(self.remove_cb)\n",
    "    return self\n",
    "\n",
    "def _grab_cbs(self, cb_cls): \n",
    "    return L(cb for cb in self.cbs if isinstance(cb, cb_cls))\n",
    "\n",
    "def remove_cb(self, cb):\n",
    "    if isinstance(cb, type): \n",
    "        self.remove_cbs(self._grab_cbs(cb))\n",
    "    else:\n",
    "        cb.learn = None\n",
    "        if hasattr(self, cb.name): \n",
    "            delattr(self, cb.name)\n",
    "        if cb in self.cbs: \n",
    "            self.cbs.remove(cb)\n",
    "    return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling Callbacks\n",
    "\n",
    "At the end of the `Learner.__init__` method, there was this line:\n",
    "\n",
    "```python\n",
    "self(\"after_create\")\n",
    "```\n",
    "which was `Learner` calling the `after_create` method of any added Callbacks. \n",
    "\n",
    "This is one of the two ways that `Learner` invokes callbacks. This works because `Learner` overrides the standard Python `__call__` method to take an `event_name`. This event name(s) is passed to `_call_one` which verifies the event is a valid callback event, sorts the callbacks, and then calls the callback it's method for that event.\n",
    "\n",
    "Note: event is a fastcore `mk_class`, which effectively works as an Enum, but as a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __call__(self, event_name): \n",
    "    L(event_name).map(self._call_one)\n",
    "\n",
    "def _call_one(self, event_name):\n",
    "    if not hasattr(event, event_name): \n",
    "        raise Exception(f'missing {event_name}')\n",
    "    for cb in self.cbs.sorted('order'): \n",
    "        cb(event_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other way `Learner` calls fastai callbacks is via the `_with_events` method. This method calls both the \"before\" and \"after\" methods for each callback event and handles Callback errors via the \"after_cancel\" Callback event. This method is used throughout the `Learner` training loop to define when callback events are called.\n",
    "\n",
    "`f` and `final` are methods we can pass to `_with_events`, and as we'll see in the next section are used to jump through the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _with_events(self, f, event_type, ex, final=noop):\n",
    "    try: \n",
    "        self(f'before_{event_type}')\n",
    "        f()\n",
    "    except ex: \n",
    "        self(f'after_cancel_{event_type}')\n",
    "    self(f'after_{event_type}')\n",
    "    final()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop Overview\n",
    "\n",
    "Once again as a reminder, this is the simplified training loop that we are recreating in a framework to be modified with Callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, valid_dl):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for xb,yb in train_dl:\n",
    "            loss = loss_func(model(xb), yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            tot_loss,tot_acc,count = 0.,0.,0\n",
    "            for xb,yb in valid_dl:\n",
    "                pred = model(xb)\n",
    "                n = len(xb)\n",
    "                count += n\n",
    "                tot_loss += loss_func(pred,yb).item()*n\n",
    "                tot_acc  += accuracy (pred,yb).item()*n\n",
    "        print(epoch, tot_loss/count, tot_acc/count)\n",
    "    return tot_loss/count, tot_acc/count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will first show the training loop method headers, so the entire fastai training loop can fit on one screen. Then we will look at each training loop method code.\n",
    "\n",
    "Remember that:\n",
    "```python\n",
    "self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
    "```\n",
    "calls any Callback's \"before_fit\", then calls `_do_fit`, handles any `CancelFitException`s, then calls any Callback's \"after_fit\", before finally calling `_end_cleanup`.\n",
    "\n",
    "For conciseness, I am leaving out the exception methods and final in the comments. You'll see them in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs optimizer, epoch, and hyperparameter setup. Then calls _with_events(_do_fit, 'fit')\n",
    "def fit(self, n_epoch, lr=None, wd=None, cbs=None, reset_opt=False, start_epoch=0):\n",
    "    pass\n",
    "\n",
    "def _do_fit(self): pass # Loops through all the epochs and calls _with_events(do_epoch, 'epoch')\n",
    "\n",
    "def _do_epoch(self): pass # Calls _do_epoch_train then _do_epoch_validate\n",
    "\n",
    "def _do_epoch_train(self): pass # Sets the Learner DataLoader to train and calls _with_events(all_batches, 'train')\n",
    "\n",
    "def all_batches(self): pass # Enumerates through all batches calling one_batch\n",
    "\n",
    "def one_batch(self, i, b): pass # Calls _set_device on the batch, optionally splits batch into Xs & Ys, and calls _with_events(_do_one_batch, 'batch')\n",
    "\n",
    "def _do_one_batch(self): pass # The tight training loop: prediction, loss, then calls _with_events(_backward, 'backward') & _with_events(self._step, 'step')\n",
    "\n",
    "def _backward(self): pass # Calls backwards on the loss\n",
    "\n",
    "def _step(self): pass # Calls the optimizer step. After this _do_one_batch calls opt.zero_grad\n",
    "\n",
    "def _do_epoch_valid(self): pass # Repeat the process on the validation set, minus the model update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop Code\n",
    "\n",
    "`fit` performs initialized the optimizer via `create_opt`, and sets hyperparameters like epoch, weight decay, etc and passes the to the optimizer. It then calls any Callback's \"before_fit\", then calls `_do_fit`, handles any `CancelFitException`s, then calls any Callback's \"after_fit\", before finally calling `_end_cleanup`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(self, n_epoch, lr=None, wd=None, cbs=None, reset_opt=False, start_epoch=0):\n",
    "    \"Fit `self.model` for `n_epoch` using `cbs`. Optionally `reset_opt`.\"\n",
    "    if start_epoch != 0:\n",
    "        cbs = L(cbs) + SkipToEpoch(start_epoch)\n",
    "    with self.added_cbs(cbs):\n",
    "        if reset_opt or not self.opt: \n",
    "            self.create_opt()\n",
    "        if wd is None: \n",
    "            wd = self.wd\n",
    "        if wd is not None: \n",
    "            self.opt.set_hypers(wd=wd)\n",
    "        self.opt.set_hypers(lr=self.lr if lr is None else lr)\n",
    "        self.n_epoch = n_epoch\n",
    "        self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`_do_fit` loops through all the epochs, calling any Callback's \"before_epoch\" method, then calls `_do_fit`, handles any `CancelEpochException`s, and finally calls any Callback's \"after_epoch\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _do_fit(self):\n",
    "    for epoch in range(self.n_epoch):\n",
    "        self.epoch=epoch\n",
    "        self._with_events(self._do_epoch, 'epoch', CancelEpochException)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`_do_epoch` calls `_do_epoch_train` then `_do_epoch_validate`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _do_epoch(self):\n",
    "    self._do_epoch_train()\n",
    "    self._do_epoch_validate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`_do_epoch_train` sets the  to train and calls any Callback's \"before_train\" method, then calls `all_batches`, handles any `CancelTrainException`s, and finally calls any Callback's \"after_train\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _do_epoch_train(self):\n",
    "    self.dl = self.dls.train\n",
    "    self._with_events(self.all_batches, 'train', CancelTrainException)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`all_batches` enumerates through the DataLoader, hopefully with multiprocessing, and calls `one_batch` with the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_batches(self):\n",
    "    self.n_iter = len(self.dl)\n",
    "    for o in enumerate(self.dl): \n",
    "        self.one_batch(*o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`one_batch` sets the batch device (if needed), splits the batch into samples and labels (if applicable) and calls any Callback's \"before_batch\" method, then calls `_do_one_batch`, handles any `CancelBatchException`s, and finally calls any Callback's \"after_batch\" method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_batch(self, i, b):\n",
    "    self.iter = i\n",
    "    b = self._set_device(b)\n",
    "    self._split(b)\n",
    "    self._with_events(self._do_one_batch, 'batch', CancelBatchException)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`_set_device` makes sure the batch is on the same device as the model is. If a batch is already on the same device, then calling `to_device` doesn't do anything too it, other than wasting a few CPU cycles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_device(self, b):\n",
    "    model_device = next(self.model.parameters()).device\n",
    "    dls_device = getattr(self.dls, 'device', default_device())\n",
    "    if model_device == dls_device: \n",
    "        return to_device(b, dls_device)\n",
    "    else: \n",
    "        return to_device(b, model_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`_split` uses the `DataLoader`'s 'n_inp' attribute (if it exists), which we set via the `DataBlock` to split the batch into inputs `xb` and labels `yb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _split(self, b):\n",
    "    i = getattr(self.dls, 'n_inp', 1 if len(b)==1 else len(b)-1)\n",
    "    self.xb,self.yb = b[:i],b[i:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`_do_one_batch` handles the direct model training loop.\n",
    "- Creates the model predictions from inputs\n",
    "- Calls any Callbacks's \"after_pred\" \n",
    "- Calculates the loss if labels exist\n",
    "- Calls any Callbacks's \"after_loss\"\n",
    "- Returns if the Learner isn't in training mode or there are no labels\n",
    "- Calls any Callback's \"before_backward\" method, then calls `backward`, handles any `CancelBackwardException`s, and finally calls any Callback's \"after_backward\" method\n",
    "- Calls any Callback's \"before_step\" method, then calls `step`, handles any `CancelStepException`s, and finally calls any Callback's \"after_step\" method\n",
    "- Finally zeroes the gradients of the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _do_one_batch(self):\n",
    "    self.pred = self.model(*self.xb)\n",
    "    self('after_pred')\n",
    "    if len(self.yb):\n",
    "        self.loss_grad = self.loss_func(self.pred, *self.yb)\n",
    "        self.loss = self.loss_grad.clone()\n",
    "    self('after_loss')\n",
    "    if not self.training or not len(self.yb): \n",
    "        return\n",
    "    self._with_events(self._backward, 'backward', CancelBackwardException)\n",
    "    self._with_events(self._step, 'step', CancelStepException)\n",
    "    self.opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`_backward` calls backward on the loss to perform the backward pass using the gradients. It is called like this with \"before_backward\" and \"after_backward\" Callback events for compatibility with Accelerate, which handles multi-GPU training for fastai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _backward(self):\n",
    "    self.loss_grad.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`_step` calls the Optimizer's step method. It is called like this with \"before_step\" and \"after_step\" Callback events so the Mixed Precision callback can emulate a PyTorch Optimizer uwhile sing the Automatic Mixed Precision `GradScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _step(self):\n",
    "    self.opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, `_do_epoch_validate` runs the whole loop again on the validation set, except without training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _do_epoch_validate(self, ds_idx=1, dl=None):\n",
    "    if dl is None: \n",
    "        dl = self.dls[ds_idx]\n",
    "    self.dl = dl\n",
    "    with torch.no_grad(): \n",
    "        self._with_events(self.all_batches, 'validate', CancelValidException)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To Be Continued\n",
    "\n",
    "To be continued after the next course lesson"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('fastai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:45:29) \n[GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13823a492421f5f84f9059b75c88ebb0eec72237eae812caa6d0bf5be399ae62"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
